---
title: 'Credit Risk Assessment: Estimating Probability of Default using Logistic Regression'
author: "Ritesh Parbhoo"
date: "2026-02-08"
output:
  html_document: default
---

Email: riteshparbhoo@outlook.com
GitHub: 


```{r, include=FALSE}
packages <- c("pROC", "PRROC", "ggplot2", "broom")
for (pkg in packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) install.packages(pkg)
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Project Description

This project, implemented in R, involves developing a credit risk model to estimate the probability of default for borrowers using logistic regression (GLM). The model examines how key variables - employment status, bank balance and annual salary - predict the probability of a counterparty defaulting on their credit obligations. The model is trained on a training set and tested on a validation set, and various methods are used to evaluate the discriminatory power of the model.

#### Motivation

Credit risk - the risk that a counterparty will default on their financial obligations - is a principal concern of lenders and financial institutions. Key to measuring credit risk is estimating probability of default which will allow institutions to make data-informed lending decisions, manage risk exposure, and comply with regulatory requirements. By developing a logistic regression model using key predictors - employment status, bank balance and annual salary - this project can identify high risk borrowers and improve credit allocation, thus reducing potential financial losses.

```{r, message=FALSE}
library(pROC)
library(PRROC)
library(ggplot2)
library(broom)
```

```{r, include=FALSE}
df <- read.csv("Default_Fin.csv")
names(df) <- c("index", "employment_status", "bank_balance", "annual_salary", "default_status")
df$employment_status <- as.factor(df$employment_status)
df$default_status <- as.factor(df$default_status)

#numeric predictors can be standardised - faster and more reliable converge of glm
df$st_bank_balance <- (df$bank_balance - mean(df$bank_balance))/sd(df$bank_balance)
df$st_annual_salary <- (df$annual_salary - mean(df$annual_salary))/sd(df$annual_salary)

var_names <- c("index", "employment_status", "st_bank_balance", "st_annual_salary", "default_status")
df <- df[, var_names]
```

## Credit Risk Dataset
#### About the Dataset

The data.frame containing the entire dataset is labeled df. The response variable, labeled "default_status" is a binary factor taking on values 0 or 1. The predictors consist of another binary factor, "employment_status", as well as two numerical values, "bank_balance" and "annual_salary", which have both been standardised and stored in the data.frame as "st_bank_balance" and "st_annual_salary" respectively.

```{r}
attach(df)
print(head(df))
print(summary(df))
```

#### Visualising the Data
```{r, echo=FALSE}
barplot(table(employment_status), main = "Barplot Depicting Prevalence of Employed Observations", col=c("powderblue", "pink"), names.arg= c("Unemployed", "Employed"))
hist(st_bank_balance, col="darkred", main="Histogram Depicting Distribution of Standardised Bank Balances" ,xlab="Standardised Bank Balance")
hist(st_bank_balance, col="darkred", main="Histogram Depicting Distribution of Standardised Annual Salaries" ,xlab="Standardised Annual Salaries")
```

As would be expected, the distributions of the standardised variables are positively skewed since before they were standardised, their values were non-negative.

#### Correlation between Variables

Analysis of the correlations between predictors and between the response and each predictor will help determine which predictors are worth including in the final fitted logistic regression model.

```{r, echo=FALSE}
#print(cor(cbind(employment_status, st_bank_balance, st_annual_salary, default_status)))


plot_transition_heatmap <- function(mat, row_col_names) {
  
  # Convert matrix to long format
  mat <- mat[rev(row_col_names), ]
  mat_long <- as.data.frame.table(mat)
  colnames(mat_long) <- c("Row", "Column", "Value")
  
  # Plot heatmap
  ggplot(mat_long, aes(x = Column, y = Row, fill = Value)) +
    geom_tile() +
    geom_text(aes(label = round(Value, 7))) +
    scale_fill_gradient(low = "green", high = "red") +
    labs(title="Variables Correlation Matrix") +
    theme_minimal() +
    theme(axis.title = element_blank(),
          axis.text = element_text(size = 10),
          axis.text.x = element_text(angle=45, hjust=1))
}

plot_transition_heatmap(cor(cbind(employment_status, st_bank_balance, st_annual_salary, default_status)), var_names[-1])
```

Of the three predictors, it is clear that the standardised bank balance shares the strongest correlation with default status, and this correlation is positive and one would expect intuitively. Employment status and annual salary share weak correlation with default status (approximately uncorrelated). Furthermore, employment status and standardised annual salary share a strong positive correlation and it may be worth removing one from the fitted logistic regression model. If either were to be removed, it makes sense that annual salary is removed as employment status is much more easily measurable.

## Fitting the Logistic Regression Models

```{r, echo=FALSE}
set.seed(234)

# Partition dataset into training and validation sets
T.rand_indices <- sort(sample(index, size = 0.8 * length(index)))
T.set <- df[T.rand_indices, ]
V.set <- df[-T.rand_indices, ]
```


The dataset of 10000 observations will be partitioned into two subsets, the training set and the validation set. The training set comprises 80% of the original dataset, selected at random. The remaining 20% of the observations make up the validation set (V.set) and will be used to evaluate the final model once it has been chosen.

```{r}
logit_model.ebs <- glm(default_status ~ employment_status + st_bank_balance + st_annual_salary, family = binomial, data = T.set)
logit_model.eb <- glm(default_status ~ employment_status + st_bank_balance, family = binomial, data=T.set)
logit_model.bs <- glm(default_status ~ st_bank_balance + st_annual_salary, family = binomial, data=T.set)
logit_model.b <-glm(default_status ~ st_bank_balance, family = binomial, data=T.set)
```

It is clear that st_bank_balance is the strongest predictor of default_status. There are four combinations of predictors that include st_bank_balance and a logistic regression model using each such combination of predictors was fitted and they will be compared. 

E := employment_status,    B := st_bank_balance,    S := st_annual_salary

#### Model Comparison

```{r, echo=FALSE}
models <- list(
  "Model 1" = logit_model.ebs,
  "Model 2" = logit_model.eb,
  "Model 3" = logit_model.bs,
  "Model 4" = logit_model.b
)

model_metrics <- data.frame(
  Residual_Deviance = sapply(models, function(m) m$deviance),
  Null_Deviance = sapply(models, function(m) m$null.deviance),
  AIC = sapply(models, function(m) m$aic),
  Predictors = c("E, B, S", "E, B", "B, S", "B")
)

print(model_metrics)

```

Null deviance is the deviance of the intercept-only model and serves as a baseline. It is the same across all four models and therefore lacks discriminative power in this case. Residual deviance measures the unexplained variation once the predictors are included; lower values indicate better fit. AIC is the most useful metric here, as it balances residual deviance with model complexity by applying a penalty for additional predictors. Model 4, which considers only st_bank_balance, is the simplest but has the poorest fit since its AIC value the greatest by a wide margin. Including at least one additional predictor to Model 4 adds predictive power. Model 2 outperforms Model 3 in both AIC and residual deviance and hence Model 2 dominates Model 3. Incorporating st_annual_salary into Model 2 to obtain Model 1 yields a marginal improvement to residual deviance, but worsens AIC. Therefore, the most parsimonious model and best-fitting model is Model 2 (employment_status, st_bank_balance).

## Best-Fitting Model: Model 2 (employment_status, st_bank_balance)
#### Model Coefficients
```{r, include=FALSE}
logit_model <- logit_model.eb
```

```{r}
pred_prob <- predict(logit_model, newdata = V.set, type="response")

tidy_model <- broom::tidy(logit_model)

# Plot coefficients
ggplot(tidy_model, aes(x = term, y = estimate)) +
  geom_col(fill = "powderblue") +
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
  coord_flip() +
  labs(title = "Logistic Regression Coefficients", y = "Coefficient Estimate", x = "Predictor")
```

In the logistic regression model, the intercept represents the log-odds of the outcome. Here the intercept is large and negative, indicating the baseline probability of default is small. Baseline in this context refers to (employment_status = 0, st_bank_balance = 0). A baseline observation is one that is unemployed and has exactly the mean bank balance of all the observations (since bank balance was standardised).

We can obtain the baseline probability of default by inputting the intercept into the log-odds (intercept) into the logistic transformation f(x) = 1/(1+e^-x).
```{r}
baseline_prob.1 <- plogis(logit_model$coefficients[1])
```

```{r, echo=FALSE}
print(paste("Baseline probability using logistic transformation on model intercept:", baseline_prob.1))
```


Reproducing the baseline probability by applying the model to an artificial baseline observation (employment_status = 0, st_bank_balance = 0):
```{r}
baseline_obs <- data.frame(employment_status = as.factor(0),
                           st_bank_balance = 0)
baseline_prob.2 <- predict(logit_model, newdata = baseline_obs, type = "response")
```

```{r, echo=FALSE}
print(paste("Baseline probability obtained by applying model to baseline observation:", baseline_prob.1))
```

#### Visualisation of Model
```{r}
tidy_model <- tidy(logit_model)

bank_seq <- seq(min(T.set$st_bank_balance), max(st_bank_balance), length.out = 100)

#ggplot(V.set, aes(x = salary_seq, y = pred_prob)) +
#  geom_point(color = "darkred", size = 1) +
#  labs(title = "Predicted Probability of Default vs Standardised Annual Salary",
#       x = "Bank Balance", y = "Predicted Probability")
```


# ROC Curve and AUC (Area under Curve)
```{r}
roc_curve <- roc(V.set$default_status, pred_prob)

plot(roc_curve, main = "ROC Curve")

auc <- auc(roc_curve)
gini <- 2*auc - 1
```

“The logistic regression model demonstrates strong discriminatory power on an out-of-sample validation set (AUC = 0.943). This performance is driven by economically intuitive predictors and a clean data-generating process. Further diagnostics focus on probability calibration and stability, as ROC metrics alone do not assess absolute PD accuracy.”









# Confusion Matrix 

```{r}
# this is done on the validation set only
# the usual cutoff 

cutoff <- length(V.set$default_status[V.set$default_status == 1])/length(V.set$default_status)

val_predictions <- ifelse(pred_prob >= cutoff, 1, 0)

confusion_matrix <- table(
  Actual = V.set$default_status,
  Predicted = val_predictions
)

# [TN FP]
# [FN TP]

```












# Precision-Recall Curve

```{r}

```












# Calibration Plots

```{r}

```







